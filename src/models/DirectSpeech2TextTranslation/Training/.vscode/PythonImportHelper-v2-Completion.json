[
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "pytorch_lightning",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pytorch_lightning",
        "description": "pytorch_lightning",
        "detail": "pytorch_lightning",
        "documentation": {}
    },
    {
        "label": "CosineAnnealingWarmRestarts",
        "importPath": "torch.optim.lr_scheduler",
        "description": "torch.optim.lr_scheduler",
        "isExtraImport": true,
        "detail": "torch.optim.lr_scheduler",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "Callback",
        "importPath": "pytorch_lightning.callbacks",
        "description": "pytorch_lightning.callbacks",
        "isExtraImport": true,
        "detail": "pytorch_lightning.callbacks",
        "documentation": {}
    },
    {
        "label": "RichProgressBar",
        "importPath": "pytorch_lightning.callbacks",
        "description": "pytorch_lightning.callbacks",
        "isExtraImport": true,
        "detail": "pytorch_lightning.callbacks",
        "documentation": {}
    },
    {
        "label": "ModelCheckpoint",
        "importPath": "pytorch_lightning.callbacks",
        "description": "pytorch_lightning.callbacks",
        "isExtraImport": true,
        "detail": "pytorch_lightning.callbacks",
        "documentation": {}
    },
    {
        "label": "StochasticWeightAveraging",
        "importPath": "pytorch_lightning.callbacks",
        "description": "pytorch_lightning.callbacks",
        "isExtraImport": true,
        "detail": "pytorch_lightning.callbacks",
        "documentation": {}
    },
    {
        "label": "RichProgressBarTheme",
        "importPath": "pytorch_lightning.callbacks.progress.rich_progress",
        "description": "pytorch_lightning.callbacks.progress.rich_progress",
        "isExtraImport": true,
        "detail": "pytorch_lightning.callbacks.progress.rich_progress",
        "documentation": {}
    },
    {
        "label": "torchmetrics.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchmetrics.functional",
        "description": "torchmetrics.functional",
        "detail": "torchmetrics.functional",
        "documentation": {}
    },
    {
        "label": "print",
        "importPath": "rich",
        "description": "rich",
        "isExtraImport": true,
        "detail": "rich",
        "documentation": {}
    },
    {
        "label": "torchaudio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchaudio",
        "description": "torchaudio",
        "detail": "torchaudio",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "yaml",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "yaml",
        "description": "yaml",
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "CLoader",
        "importPath": "yaml",
        "description": "yaml",
        "isExtraImport": true,
        "detail": "yaml",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "join",
        "importPath": "os.path",
        "description": "os.path",
        "isExtraImport": true,
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "TokenHandler",
        "importPath": "aang.utils.lang",
        "description": "aang.utils.lang",
        "isExtraImport": true,
        "detail": "aang.utils.lang",
        "documentation": {}
    },
    {
        "label": "normalize_digits",
        "importPath": "pyarabic.trans",
        "description": "pyarabic.trans",
        "isExtraImport": true,
        "detail": "pyarabic.trans",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "importPath": "tokenizers",
        "description": "tokenizers",
        "isExtraImport": true,
        "detail": "tokenizers",
        "documentation": {}
    },
    {
        "label": "setuptools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "setuptools",
        "description": "setuptools",
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "DDPStrategy",
        "importPath": "pytorch_lightning.strategies",
        "description": "pytorch_lightning.strategies",
        "isExtraImport": true,
        "detail": "pytorch_lightning.strategies",
        "documentation": {}
    },
    {
        "label": "MuSTCDataset",
        "importPath": "aang.utils.data",
        "description": "aang.utils.data",
        "isExtraImport": true,
        "detail": "aang.utils.data",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "aang.utils.callback",
        "description": "aang.utils.callback",
        "isExtraImport": true,
        "detail": "aang.utils.callback",
        "documentation": {}
    },
    {
        "label": "Speech2TextArcht",
        "importPath": "aang.model.S2T",
        "description": "aang.model.S2T",
        "isExtraImport": true,
        "detail": "aang.model.S2T",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "ArgumentParser",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "HPrams",
        "kind": 6,
        "importPath": "aang.model.S2T",
        "description": "aang.model.S2T",
        "peekOfCode": "class HPrams:\n    wave_param: dict     = field(default_factory=dict)\n    encoder_params: dict = field(default_factory=dict)\n    decoder_params: dict = field(default_factory=dict)\n    head_names: dict     = field(default_factory=dict)\n    head_params: dict    = field(default_factory=dict)\nclass Model(nn.Module):  \n    def __init__(self, wave_param: dict, encoder_params: dict, decoder_params: dict, head_names: dict, head_params: dict):\n        super(Model, self).__init__()\n        self.hparams = HPrams(wave_param=wave_param, encoder_params=encoder_params, decoder_params=decoder_params, ",
        "detail": "aang.model.S2T",
        "documentation": {}
    },
    {
        "label": "Model",
        "kind": 6,
        "importPath": "aang.model.S2T",
        "description": "aang.model.S2T",
        "peekOfCode": "class Model(nn.Module):  \n    def __init__(self, wave_param: dict, encoder_params: dict, decoder_params: dict, head_names: dict, head_params: dict):\n        super(Model, self).__init__()\n        self.hparams = HPrams(wave_param=wave_param, encoder_params=encoder_params, decoder_params=decoder_params, \n                              head_names=head_names, head_params=head_params)\n        self.wave_enc = Wave2Chunk(**self.hparams.wave_param)\n        self.context_enc = Encoder(**self.hparams.encoder_params)\n        self.context_dec = Decoder(**self.hparams.decoder_params)\n        self.heads = nn.ModuleDict()\n        for h, pad_idx in self.hparams.head_names.items():",
        "detail": "aang.model.S2T",
        "documentation": {}
    },
    {
        "label": "Speech2TextArcht",
        "kind": 6,
        "importPath": "aang.model.S2T",
        "description": "aang.model.S2T",
        "peekOfCode": "class Speech2TextArcht(pl.LightningModule):\n    def __init__(self, wave_param: dict, encoder_params: dict, decoder_params: dict, head_names: list, head_params: dict, lr: float):\n        super(Speech2TextArcht, self).__init__()\n        self.save_hyperparameters()\n        self.model = Model(wave_param, encoder_params, decoder_params, head_names, head_params)\n        self.model.init_weights()\n#         torch._dynamo.reset()\n#         self.model = torch.compile(self.model, dynamic=True)\n    def forward(self, wave, target: dict, training=False, wave_mask=False, need_enc_weights=False, enc_mask=False,\n                target_mask=False, need_dec_weights=False, dec_mask=False):",
        "detail": "aang.model.S2T",
        "documentation": {}
    },
    {
        "label": "EncoderLayer",
        "kind": 6,
        "importPath": "aang.modules.former",
        "description": "aang.modules.former",
        "peekOfCode": "class EncoderLayer(nn.Module):\n    def __init__(self, **kwargs):\n        \"\"\"@params:\n                    d_model, nhead, nch, dropout=0.3, batch_first=True\n        \"\"\"\n        super(EncoderLayer, self).__init__()\n        self.config = kwargs        \n        self.pre_norm = nn.LayerNorm(self.config['d_model'])\n        self.mha      = nn.MultiheadAttention(embed_dim=self.config['d_model'], num_heads=self.config['nhead'], dropout=self.config['dropout'], \n                                               bias=True, add_bias_kv=True, batch_first=self.config['batch_first'])",
        "detail": "aang.modules.former",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "aang.modules.former",
        "description": "aang.modules.former",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(self, **kwargs):\n        '''d_model=512, nhead=4, nch=5, dropout=0.3, batch_first=True, size'''\n        super().__init__()\n        self.config = kwargs\n        assert self.config['d_model'] % self.config['nhead'] == 0, 'd_model should be dvisible by num of heads'\n        self.enc_stack = nn.ModuleList([EncoderLayer(d_model=self.config['d_model'], nch=self.config['nch'], nhead=self.config['nhead'], \n                                                     dropout=self.config['dropout'], batch_first=self.config['batch_first']) \n                                        for _ in range(self.config['size'])])\n    def forward(self, x, padding_mask=None, need_weights=False, training=False):",
        "detail": "aang.modules.former",
        "documentation": {}
    },
    {
        "label": "DecoderLayer",
        "kind": 6,
        "importPath": "aang.modules.former",
        "description": "aang.modules.former",
        "peekOfCode": "class DecoderLayer(nn.Module):\n    def __init__(self, **kwargs):\n        \"\"\"@params:\n                    d_model, nhead, nch, dropout=0.3, batch_first=True\n        \"\"\"\n        super(DecoderLayer, self).__init__()\n        self.config = kwargs\n        self.pre_norm1 = nn.LayerNorm(self.config['d_model'])\n        self.smha      = nn.MultiheadAttention(embed_dim=self.config['d_model'], num_heads=self.config['nhead'], dropout=self.config['dropout'], \n                                               bias=True, add_bias_kv=True, batch_first=self.config['batch_first'])",
        "detail": "aang.modules.former",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "aang.modules.former",
        "description": "aang.modules.former",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(self, **kwargs):\n        '''d_model=512, nhead=4, nch=5, dropout=0.3, batch_first=True, size'''\n        super().__init__()\n        self.config = kwargs\n        assert self.config['d_model'] % self.config['nhead'] == 0, 'd_model should be dvisible by num of heads'\n        self.dec_stack = nn.ModuleList([DecoderLayer(d_model=self.config['d_model'], nch=self.config['nch'], nhead=self.config['nhead'], \n                                                     dropout=self.config['dropout'], batch_first=self.config['batch_first']) \n                                        for _ in range(self.config['size'])])\n    def forward(self, query, key, query_mask=None, key_mask=None, need_weights=False, training=False):",
        "detail": "aang.modules.former",
        "documentation": {}
    },
    {
        "label": "Head",
        "kind": 6,
        "importPath": "aang.modules.heads",
        "description": "aang.modules.heads",
        "peekOfCode": "class Head(nn.Module):\n    def __init__(self, **kwargs):\n        '''d_model, voc_size'''\n        super().__init__()\n        self.config = kwargs\n        self.layer1 =  nn.Sequential(nn.Linear(self.config['d_model'], self.config['voc_size'] // 3),\n                                     nn.Tanhshrink())\n        self.norm1 = nn.BatchNorm1d(self.config['voc_size'] // 3)\n        self.layer2 = nn.Sequential(nn.Dropout(0.5),\n                                    nn.Linear(self.config['voc_size'] // 3, self.config['voc_size']))",
        "detail": "aang.modules.heads",
        "documentation": {}
    },
    {
        "label": "Wave2Chunk",
        "kind": 6,
        "importPath": "aang.modules.wave",
        "description": "aang.modules.wave",
        "peekOfCode": "class Wave2Chunk(nn.Module):\n    def __init__(self, **kwargs):\n        '''frame_size, frame_stride, b1, b2, b3, b4, out_dim=512'''\n        super(Wave2Chunk, self).__init__()\n        self.config = kwargs\n        assert self.config['frame_size'] >= 2000, \"the frame size minimum should be greater than or equal 2000\"\n        assert not self.config['frame_size'] % self.config['b4'], 'frame size should be divisible by num of fiinal chanels'\n        pram_per_layer = [(19, 5, 0, 3), (19, 5, 0, 2), (2, 2, 0, 1),\n                          (7, 1, 0, 1), (7, 2, 0, 1), (3, 3, 0, 1),\n                          (5, 2, 0, 1), (5, 2, 0, 1), (2, 1, 0, 1),   ",
        "detail": "aang.modules.wave",
        "documentation": {}
    },
    {
        "label": "Predictions",
        "kind": 6,
        "importPath": "aang.utils.callback",
        "description": "aang.utils.callback",
        "peekOfCode": "class Predictions(Callback):\n    def __init__(self, config):\n        self.tokenizers = dict()\n        for k, v in config.items():\n            self.tokenizers[k] = TokenHandler(v, k)\n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, *args, **kwargs):\n        if not batch_idx % 100:\n            wave, en, ar = batch\n            ground_truth = {'en': en[:, 1:], 'ar': ar[:, 1:]}\n            with torch.no_grad():",
        "detail": "aang.utils.callback",
        "documentation": {}
    },
    {
        "label": "progress_bar",
        "kind": 5,
        "importPath": "aang.utils.callback",
        "description": "aang.utils.callback",
        "peekOfCode": "progress_bar = RichProgressBar(theme=RichProgressBarTheme(description=\"green_yellow\",\n                                                          progress_bar=\"green1\",\n                                                          progress_bar_finished=\"green1\",\n                                                          progress_bar_pulse=\"#6206E0\",\n                                                          batch_progress=\"green_yellow\",\n                                                          time=\"grey82\",\n                                                          processing_speed=\"grey82\",\n                                                          metrics=\"green1\",\n                                                        ))\nckp = ModelCheckpoint(every_n_train_steps=1000, save_last=True, auto_insert_metric_name=False)",
        "detail": "aang.utils.callback",
        "documentation": {}
    },
    {
        "label": "ckp",
        "kind": 5,
        "importPath": "aang.utils.callback",
        "description": "aang.utils.callback",
        "peekOfCode": "ckp = ModelCheckpoint(every_n_train_steps=1000, save_last=True, auto_insert_metric_name=False)\nswa = StochasticWeightAveraging(swa_lrs=1e-2, annealing_epochs=2)\nif __name__ == '__main__':\n    pass",
        "detail": "aang.utils.callback",
        "documentation": {}
    },
    {
        "label": "swa",
        "kind": 5,
        "importPath": "aang.utils.callback",
        "description": "aang.utils.callback",
        "peekOfCode": "swa = StochasticWeightAveraging(swa_lrs=1e-2, annealing_epochs=2)\nif __name__ == '__main__':\n    pass",
        "detail": "aang.utils.callback",
        "documentation": {}
    },
    {
        "label": "DFMap",
        "kind": 6,
        "importPath": "aang.utils.data",
        "description": "aang.utils.data",
        "peekOfCode": "class DFMap(Dataset):\n    def __init__(self, df, ar_config: dict, en_config: dict, wav_config, **kwargs):\n        self.df = df\n        self.ar_config  = ar_config.copy()\n        self.en_config  = en_config.copy()\n        self.wav_config = wav_config.copy()\n        self.ar_config['tokenizer'] = TokenHandler(self.ar_config['tokenizer'], 'ar')\n        self.en_config['tokenizer'] = TokenHandler(self.en_config['tokenizer'], 'en')\n    def __len__(self):\n        return len(self.df)",
        "detail": "aang.utils.data",
        "documentation": {}
    },
    {
        "label": "MuSTCDataset",
        "kind": 6,
        "importPath": "aang.utils.data",
        "description": "aang.utils.data",
        "peekOfCode": "class MuSTCDataset(pl.LightningDataModule):\n    def __init__(self, data_config, loader_config=None):\n        super().__init__()\n        self.data_config = data_config\n        self.loader_config = loader_config\n    def setup(self, stage: str):\n        self.data = self.load_from_dir(self.data_config['dir_path'])\n    def load_from_dir(self, root):\n        # split_name = root.split(os.sep)[-1]\n        split_names = ['train', 'dev']",
        "detail": "aang.utils.data",
        "documentation": {}
    },
    {
        "label": "TokenHandler",
        "kind": 6,
        "importPath": "aang.utils.lang",
        "description": "aang.utils.lang",
        "peekOfCode": "class TokenHandler:\n    def __init__(self, json_path: str, lang='en', ):\n        self.tok = Tokenizer.from_file(json_path)\n        self.tok.enable_padding(pad_id=self.get_id(\"<PAD>\"), pad_token=\"<PAD>\")\n        if lang == 'en':\n            self.pre = self.english_preprocess\n        elif lang == 'ar':\n            self.pre = self.arabic_preprocess\n        else:\n            raise NotImplementedError('This class suports En and Ar language only for now')",
        "detail": "aang.utils.lang",
        "documentation": {}
    }
]